name: Benchmark

on:
  workflow_dispatch:
  push:
    branches:
      - main

jobs:
  benchmark:
    name: Performance regression check
    runs-on: self-hosted

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Rust
        uses: dtolnay/rust-toolchain@stable
        with:
          components: rustfmt, clippy

      - name: Cache Rust dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/bin/
            ~/.cargo/registry/index/
            ~/.cargo/registry/cache/
            ~/.cargo/git/db/
            gridcore-rs/target/
          key: ${{ runner.os }}-cargo-${{ hashFiles('**/Cargo.lock') }}
          restore-keys: |
            ${{ runner.os }}-cargo-

      - name: Install cargo-criterion
        run: |
          cargo install cargo-criterion --locked || true

      - name: Run benchmarks
        run: |
          cd gridcore-rs
          cargo bench --all --no-fail-fast -- --save-baseline current
          
          # Generate JSON output for benchmark-action
          cargo criterion --message-format=json > ../benchmark-results.json || true
          
          # If cargo-criterion doesn't work, use criterion's own output
          if [ ! -s ../benchmark-results.json ]; then
            echo "Creating benchmark results from criterion output..."
            # Parse criterion results into JSON format
            python3 -c "
import json
import os
import re
from pathlib import Path

results = []
criterion_dir = Path('target/criterion')

if criterion_dir.exists():
    for bench_dir in criterion_dir.glob('*'):
        if bench_dir.is_dir() and not bench_dir.name.startswith('.'):
            estimates_file = bench_dir / 'base' / 'estimates.json'
            if estimates_file.exists():
                with open(estimates_file) as f:
                    data = json.load(f)
                    # Convert to format expected by benchmark-action
                    results.append({
                        'name': bench_dir.name,
                        'unit': 'ns/iter',
                        'value': data.get('mean', {}).get('point_estimate', 0) * 1e9 if 'mean' in data else 0
                    })

output = {
    'results': results
}

with open('../benchmark-results.json', 'w') as f:
    json.dump(output, f, indent=2)
print(f'Generated benchmark results with {len(results)} benchmarks')
            " || echo "[]" > ../benchmark-results.json
          fi
          
          # Verify the file was created
          ls -la ../benchmark-results.json

      - name: Download previous benchmark data
        uses: actions/cache@v4
        with:
          path: ./cache
          key: ${{ runner.os }}-rust-benchmark

      - name: Store benchmark result
        uses: benchmark-action/github-action-benchmark@v1
        with:
          tool: "customSmallerIsBetter"
          output-file-path: benchmark-results.json
          external-data-json-path: ./cache/benchmark-data.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          fail-on-alert: true
          alert-threshold: "110%"
          comment-on-alert: true
          alert-comment-cc-users: "@vinay"

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: |
            benchmark-results.json
            gridcore-rs/target/criterion/
          retention-days: 30

